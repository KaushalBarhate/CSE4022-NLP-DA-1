{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20BCE1099\n",
    "### Kaushal Nilesh Barhate\n",
    "### CSE4022 E1+TE1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Utilize Python NLTK (Natural Language Tool Kit) Platform and do the following.\n",
    "Install relevant Packages and Libraries\n",
    "• Explore Brown Corpus and find the size, tokens, categories,\n",
    "• Find the size of word tokens?\n",
    "• Find the size of word types?\n",
    "• Find the size of the category “government”\n",
    "• List the most frequent tokens\n",
    "• Count the number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 475.4 kB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ------------------------------------ 298.0/298.0 kB 511.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\barha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\barha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\barha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\barha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Installing collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.2.0 nltk-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\barha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Brown Corpus: 500\n",
      "Categories in Brown Corpus: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "Number of tokens: 1161192\n",
      "Number of unique types: 56057\n",
      "Number of tokens in the 'government' category: 70117\n",
      "Most frequent tokens: [('the', 62713), (',', 58334), ('.', 49346), ('of', 36080), ('and', 27915), ('to', 25732), ('a', 21881), ('in', 19536), ('that', 10237), ('is', 10011)]\n",
      "Number of sentences: 57340\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "print(\"Size of Brown Corpus:\", len(brown.fileids()))\n",
    "print(\"Categories in Brown Corpus:\", brown.categories())\n",
    "print(\"Number of tokens:\", len(brown.words()))\n",
    "print(\"Number of unique types:\", len(set(brown.words())))\n",
    "government_text = brown.words(categories=['government'])\n",
    "print(\"Number of tokens in the 'government' category:\", len(government_text))\n",
    "\n",
    "fd = nltk.FreqDist(brown.words())\n",
    "print(\"Most frequent tokens:\", fd.most_common(10))\n",
    "print(\"Number of sentences:\", len(brown.sents()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explore the corpora available in NLTK (any two) (02 Marks)\n",
    "• Raw corpus\n",
    "• POS tagged\n",
    "• Parsed\n",
    "• Multilingual aligned\n",
    "• Spoken language\n",
    "• Semantic tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'kaushal', 'barhate', '.', 'lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', ',', 'sed', 'eiusmod', 'tempor', 'incididunt', 'ut', 'labore', 'et', 'dolore', 'magna', 'aliqua', '.', 'ut', 'enim', 'ad', 'minim', 'veniam', ',', 'quis', 'nostrud', 'exercitation', 'ullamco', 'laboris', 'nisi', 'ut', 'aliquip', 'ex', 'ea', 'commodo', 'consequat', '.', 'duis', 'aute', 'irure', 'dolor', 'reprehenderit', 'voluptate', 'velit', 'esse', 'cillum', 'dolore', 'eu', 'fugiat', 'nulla', 'pariatur', '.', 'excepteur', 'sint', 'occaecat', 'cupidatat', 'non', 'proident', ',', 'sunt', 'culpa', 'qui', 'officia', 'deserunt', 'mollit', 'anim', 'id', 'est', 'laborum', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\barha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Raw corpus\n",
    "\n",
    "raw_corpus= \"Hello this is Kaushal Barhate. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "tokens = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "# Lowercase all the tokens\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "tokens = [token for token in tokens if token not in stopwords]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('Kaushal', 'NNP'),\n",
       " ('Barhate', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Lorem', 'NNP'),\n",
       " ('ipsum', 'JJ'),\n",
       " ('dolor', 'NN'),\n",
       " ('sit', 'NN'),\n",
       " ('amet', 'NN'),\n",
       " (',', ','),\n",
       " ('consectetur', 'NN'),\n",
       " ('adipiscing', 'VBG'),\n",
       " ('elit', 'NN'),\n",
       " (',', ','),\n",
       " ('sed', 'VBN'),\n",
       " ('do', 'VBP'),\n",
       " ('eiusmod', 'VB'),\n",
       " ('tempor', 'VB'),\n",
       " ('incididunt', 'NN'),\n",
       " ('ut', 'JJ'),\n",
       " ('labore', 'NN'),\n",
       " ('et', 'NN'),\n",
       " ('dolore', 'NN'),\n",
       " ('magna', 'NN'),\n",
       " ('aliqua', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Ut', 'NNP'),\n",
       " ('enim', 'JJ'),\n",
       " ('ad', 'NN'),\n",
       " ('minim', 'NN'),\n",
       " ('veniam', 'NN'),\n",
       " (',', ','),\n",
       " ('quis', 'JJ'),\n",
       " ('nostrud', 'JJ'),\n",
       " ('exercitation', 'NN'),\n",
       " ('ullamco', 'JJ'),\n",
       " ('laboris', 'NN'),\n",
       " ('nisi', 'JJ'),\n",
       " ('ut', 'JJ'),\n",
       " ('aliquip', 'NN'),\n",
       " ('ex', 'NN'),\n",
       " ('ea', 'FW'),\n",
       " ('commodo', 'NN'),\n",
       " ('consequat', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Duis', 'NNP'),\n",
       " ('aute', 'JJ'),\n",
       " ('irure', 'NN'),\n",
       " ('dolor', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('reprehenderit', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('voluptate', 'NN'),\n",
       " ('velit', 'NN'),\n",
       " ('esse', 'NN'),\n",
       " ('cillum', 'NN'),\n",
       " ('dolore', 'NN'),\n",
       " ('eu', 'JJ'),\n",
       " ('fugiat', 'NN'),\n",
       " ('nulla', 'NN'),\n",
       " ('pariatur', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Excepteur', 'NNP'),\n",
       " ('sint', 'NN'),\n",
       " ('occaecat', 'NN'),\n",
       " ('cupidatat', 'NN'),\n",
       " ('non', 'JJ'),\n",
       " ('proident', 'NN'),\n",
       " (',', ','),\n",
       " ('sunt', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('culpa', 'NN'),\n",
       " ('qui', 'NN'),\n",
       " ('officia', 'NN'),\n",
       " ('deserunt', 'NN'),\n",
       " ('mollit', 'NN'),\n",
       " ('anim', 'NN'),\n",
       " ('id', 'NN'),\n",
       " ('est', 'JJS'),\n",
       " ('laborum', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS Tagging\n",
    "# The NLTK library contains a variety of corpora that have undergone different types of annotation, including POS tagging. An example of this is the Penn Treebank corpus, which is composed of over 4.5 million words of text, mainly from Wall Street Journal articles, with each word being labeled with its grammatical role. The POS tags for each word can be accessed using the tagged_words() method. These POS tagged corpora can be used for various tasks, such as syntactic parsing and grammatical analysis, among others\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "text = \"Hello this is Kaushal Barhate. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "tokens_in_text = word_tokenize(text)\n",
    "\n",
    "nltk.pos_tag(tokens_in_text)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a text corpus with a minimum of 200 words (unique content). Implement the\n",
    "following text processing\n",
    "• Word segmentation\n",
    "• Sentence segmentation\n",
    "• Convert to Lowercase\n",
    "• Stop words removal\n",
    "• Stemming\n",
    "• Lemmatization\n",
    "• Part of speech tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\barha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\barha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\barha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of words: 259\n",
      "[('lorem', 'NN'), ('ipsum', 'NN'), ('dolor', 'NN'), ('sit', 'NN'), ('amet', 'NN'), (',', ','), ('consectetur', 'NN'), ('adipisc', 'NN'), ('elit', 'NN'), ('.', '.'), ('sed', 'VBN'), ('malesuada', 'NN'), (',', ','), ('magna', 'FW'), ('id', 'JJ'), ('laoreet', 'NN'), ('bibendum', 'NN'), (',', ','), ('sem', 'NN'), ('ant', 'JJ'), ('congu', 'NN'), ('risu', 'NN'), (',', ','), ('vel', 'NN'), ('malesuada', 'NN'), ('massa', 'NN'), ('magna', 'NN'), ('qui', 'NN'), ('odio', 'NN'), ('.', '.'), ('sed', 'VBN'), ('id', 'JJ'), ('lorem', 'NN'), ('risu', 'NN'), ('volutpat', 'NN'), ('molesti', 'NN'), ('.', '.'), ('sed', 'VBN'), ('euismod', 'NN'), (',', ','), ('velit', 'FW'), ('vel', 'FW'), ('gravida', 'FW'), ('congu', 'NN'), (',', ','), ('velit', 'NN'), ('est', 'JJS'), ('eleifend', 'NN'), ('augu', 'NN'), (',', ','), ('id', 'JJ'), ('conval', 'NN'), ('risu', 'NN'), ('nibh', 'NN'), ('non', 'NN'), ('risu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('euismod', 'JJ'), ('aliquet', 'NN'), ('nibh', 'NN'), (',', ','), ('eu', 'JJ'), ('faucibu', 'NN'), ('risu', 'NN'), ('elementum', 'NN'), ('id', 'NN'), ('.', '.'), ('sed', 'VBN'), ('id', 'JJ'), ('rutrum', 'NN'), ('augu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('feugiat', 'NN'), (',', ','), ('velit', 'FW'), ('vel', 'FW'), ('gravida', 'FW'), ('congu', 'NN'), (',', ','), ('velit', 'NN'), ('est', 'JJS'), ('eleifend', 'NN'), ('augu', 'NN'), (',', ','), ('id', 'JJ'), ('conval', 'NN'), ('risu', 'NN'), ('nibh', 'NN'), ('non', 'NN'), ('risu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('euismod', 'JJ'), ('aliquet', 'NN'), ('nibh', 'NN'), (',', ','), ('eu', 'JJ'), ('faucibu', 'NN'), ('risu', 'NN'), ('elementum', 'NN'), ('id', 'NN'), ('.', '.'), ('sed', 'VBN'), ('id', 'JJ'), ('rutrum', 'NN'), ('augu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('feugiat', 'NN'), (',', ','), ('velit', 'FW'), ('vel', 'FW'), ('gravida', 'FW'), ('congu', 'NN'), (',', ','), ('velit', 'NN'), ('est', 'JJS'), ('eleifend', 'NN'), ('augu', 'NN'), (',', ','), ('id', 'JJ'), ('conval', 'NN'), ('risu', 'NN'), ('nibh', 'NN'), ('non', 'NN'), ('risu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('euismod', 'JJ'), ('aliquet', 'NN'), ('nibh', 'NN'), (',', ','), ('eu', 'JJ'), ('faucibu', 'NN'), ('risu', 'NN'), ('elementum', 'NN'), ('id', 'NN'), ('.', '.'), ('sed', 'VBN'), ('id', 'JJ'), ('rutrum', 'NN'), ('augu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('feugiat', 'NN'), (',', ','), ('velit', 'FW'), ('vel', 'FW'), ('gravida', 'FW'), ('congu', 'NN'), (',', ','), ('velit', 'NN'), ('est', 'JJS'), ('eleifend', 'NN'), ('augu', 'NN'), (',', ','), ('id', 'JJ'), ('conval', 'NN'), ('risu', 'NN'), ('nibh', 'NN'), ('non', 'NN'), ('risu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('euismod', 'JJ'), ('aliquet', 'NN'), ('nibh', 'NN'), (',', ','), ('eu', 'JJ'), ('faucibu', 'NN'), ('risu', 'NN'), ('elementum', 'NN'), ('id', 'NN'), ('.', '.'), ('sed', 'VBN'), ('id', 'JJ'), ('rutrum', 'NN'), ('augu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('feugiat', 'NN'), (',', ','), ('velit', 'FW'), ('vel', 'FW'), ('gravida', 'FW'), ('congu', 'NN'), (',', ','), ('velit', 'NN'), ('est', 'JJS'), ('eleifend', 'NN'), ('augu', 'NN'), (',', ','), ('id', 'JJ'), ('conval', 'NN'), ('risu', 'NN'), ('nibh', 'NN'), ('non', 'NN'), ('risu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('euismod', 'JJ'), ('aliquet', 'NN'), ('nibh', 'NN'), (',', ','), ('eu', 'JJ'), ('faucibu', 'NN'), ('risu', 'NN'), ('elementum', 'NN'), ('id', 'NN'), ('.', '.'), ('sed', 'VBN'), ('id', 'JJ'), ('rutrum', 'NN'), ('augu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('feugiat', 'NN'), (',', ','), ('velit', 'FW'), ('vel', 'FW'), ('gravida', 'FW'), ('congu', 'NN'), (',', ','), ('velit', 'NN'), ('est', 'JJS'), ('eleifend', 'NN'), ('augu', 'NN'), (',', ','), ('id', 'JJ'), ('conval', 'NN'), ('risu', 'NN'), ('nibh', 'NN'), ('non', 'NN'), ('risu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('euismod', 'JJ'), ('aliquet', 'NN'), ('nibh', 'NN'), (',', ','), ('eu', 'JJ'), ('faucibu', 'NN'), ('risu', 'NN'), ('elementum', 'NN'), ('id', 'NN'), ('.', '.'), ('sed', 'VBN'), ('id', 'JJ'), ('rutrum', 'NN'), ('augu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('feugiat', 'NN'), (',', ','), ('velit', 'FW'), ('vel', 'FW'), ('gravida', 'FW'), ('congu', 'NN'), (',', ','), ('velit', 'NN'), ('est', 'JJS'), ('eleifend', 'NN'), ('augu', 'NN'), (',', ','), ('id', 'JJ'), ('conval', 'NN'), ('risu', 'NN'), ('nibh', 'NN'), ('non', 'NN'), ('risu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('euismod', 'JJ'), ('aliquet', 'NN'), ('nibh', 'NN'), (',', ','), ('eu', 'JJ'), ('faucibu', 'NN'), ('risu', 'NN'), ('elementum', 'NN'), ('id', 'NN'), ('.', '.'), ('sed', 'VBN'), ('id', 'JJ'), ('rutrum', 'NN'), ('augu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('feugiat', 'NN'), (',', ','), ('velit', 'FW'), ('vel', 'FW'), ('gravida', 'FW'), ('congu', 'NN'), (',', ','), ('velit', 'NN'), ('est', 'JJS'), ('eleifend', 'NN'), ('augu', 'NN'), (',', ','), ('id', 'JJ'), ('conval', 'NN'), ('risu', 'NN'), ('nibh', 'NN'), ('non', 'NN'), ('risu', 'NN'), ('.', '.'), ('sed', 'VBN'), ('euismod', 'JJ'), ('aliquet', 'NN'), ('nibh', 'NN'), (',', ','), ('eu', 'JJ'), ('faucibu', 'NN'), ('risu', 'NN'), ('elementum', 'NN'), ('id', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a text corpus with at least 200 words\n",
    "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed malesuada, magna id laoreet bibendum, sem ante congue risus, vel malesuada massa magna quis odio. Sed id lorem a risus volutpat molestie. Sed euismod, velit vel gravida congue, velit est eleifend augue, id convallis risus nibh non risus. Sed euismod aliquet nibh, eu faucibus risus elementum id. Sed id rutrum augue. Sed feugiat, velit vel gravida congue, velit est eleifend augue, id convallis risus nibh non risus. Sed euismod aliquet nibh, eu faucibus risus elementum id. Sed id rutrum augue. Sed feugiat, velit vel gravida congue, velit est eleifend augue, id convallis risus nibh non risus. Sed euismod aliquet nibh, eu faucibus risus elementum id. Sed id rutrum augue. Sed feugiat, velit vel gravida congue, velit est eleifend augue, id convallis risus nibh non risus. Sed euismod aliquet nibh, eu faucibus risus elementum id. Sed id rutrum augue. Sed feugiat, velit vel gravida congue, velit est eleifend augue, id convallis risus nibh non risus. Sed euismod aliquet nibh, eu faucibus risus elementum id. Sed id rutrum augue. Sed feugiat, velit vel gravida congue, velit est eleifend augue, id convallis risus nibh non risus. Sed euismod aliquet nibh, eu faucibus risus elementum id. Sed id rutrum augue. Sed feugiat, velit vel gravida congue, velit est eleifend augue, id convallis risus nibh non risus. Sed euismod aliquet nibh, eu faucibus risus elementum id. Sed id rutrum augue. Sed feugiat, velit vel gravida congue, velit est eleifend augue, id convallis risus nibh non risus. Sed euismod aliquet nibh, eu faucibus risus elementum id.\"\n",
    "print(\"length of words: \"+ str(len(text.split())))\n",
    "\n",
    "# Word segmentation\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Sentence segmentation\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Convert to lowercase\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "tokens = [ps.stem(token) for token in tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Part of speech tagger\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68345db527c63fa982c42ab74acf3475189cd5793b830a145cf7847d50fbf04a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
